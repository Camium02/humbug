Context: Implementation
    Context: Communication with the AI
        Communication with the AI is via a REST API.

        The implementation supports OpenAI, Google Gemini, and Anthropic endpoints.

        Context: OpenAI REST endpoint
            Messages are sent via a POST message.

            The API to use is the "continuations" API.  The endpoint is "https://api.openai.com/v1/chat/completions".

            This needs two headers to be provided:

            "Content-Type": "application/json"
            "Authorization": {key}"

            {key} must be replaced with the API key found in the environment variable `OPENAI_API_KEY`.

            The data for the POST is of this form:

            ```json
            {
                "model": "selected-model",
                "messages": [{"role": "user", "content": "Say this is a test!"}],
                "temperature": temp-setting,
                "stream": true,
                "stream_options": {"include_usage": true}
            }
            ```

            In this, "selected-model" is name of the selected AI model, and "temp-setting" is the numeric temperature
            setting defined by the user.

            The "content" section of a message should be replaced with the user's message to the AI.
            With streaming enabled, responses will arrive as chunks. Each chunk begins with "data: " followed
            by a JSON object. This repeats until a chunk containing "data: [DONE]" arrives.  Each JSON chunk
            takes this form:

            ```json
            {
                "id": "chatcmpl-abc123",
                "object": "chat.completion.chunk",
                "created": 1677858242,
                "model": "gpt-4o-mini",
                "choices": [
                    {
                        "delta": {
                            "content": "chunk of text"
                        },
                        "finish_reason": null,
                        "index": 0
                    }
                ]
            }
            ```

            The final chunk includes usage information:

            ```json
            {
                "id": "chatcmpl-abc123",
                "object": "chat.completion.chunk",
                "created": 1677858242,
                "model": "gpt-4o-mini",
                "usage": {
                    "prompt_tokens": 13,
                    "completion_tokens": 7,
                    "total_tokens": 20
                },
                "choices": [
                    {
                        "delta": {},
                        "finish_reason": "stop",
                        "index": 0
                    }
                ]
            }
            ```

            In this message, the content is what should be captured as the AI response, but capture the usage information
            in the JSON transcript.

        Context: Google REST endpoint
            Messages are sent via a POST message.

            The API to use is the Gemini streaming API. The endpoint pattern is:

            "https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent?alt=sse"

            where {model} is replaced with the selected model name (e.g., "gemini-1.5-flash").

            This needs two headers to be provided:

            "Content-Type": "application/json"
            "x-goog-api-key": {key}

            {key} must be replaced with the API key found in the environment variable `GOOGLE_API_KEY`.

            The data for the POST is of this form:

            ```json
            {
                "contents": [
                    {"role":"user",
                        "parts":[{
                            "text": "Hello"
                        }]
                    },
                    {"role": "model",
                        "parts":[{
                            "text": "Great to meet you. What would you like to know?"
                        }]
                    },
                    {"role":"user",
                        "parts":[{
                            "text": "I have two dogs in my house. How many paws are in my house?"
                        }]
                    },
                ],
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_ONLY_HIGH"
                    }
                ],
                "generationConfig": {
                    "temperature": 0.7,
                    "topP": 0.8,
                    "topK": 10
                }
            }

            ```

            The temperature value should be omitted for models that don't support it.

            With streaming enabled (alt=sse parameter), responses arrive as Server-Sent Events.

            Each event begins with "data: " followed by a JSON object.

            The stream ends with a "data: [DONE]" message.

            Each JSON chunk takes this form:

            ```json
            {
                "candidates": [
                    {
                        "content": {
                            "parts": [
                                {
                                    "text": "chunk of response text"
                                }
                            ]
                        },
                        "finishReason": null
                    }
                ]
            }
            ```

            The final chunk will have finishReason set to "STOP" and may include usage information.

        Context: Anthropic REST endpoint
            Messages are sent via a POST message.

            The endpoint is "https://api.anthropic.com/v1/messages".

            This needs three headers to be provided:

            "Content-Type": "application/json"
            "x-api-key": {key}
            "anthropic-version": "2023-06-01"

            {key} must be replaced with the API key found in the environment variable `ANTHROPIC_API_KEY`.

            The data for the POST is of this form:

            ```json
            {
                "model": "selected-model",
                "messages": [{"role": "user", "content": "Say this is a test!"}],
                "temperature": temp-setting,
                "max_tokens": 1024,
                "stream": true
            }
            ```

            In this, "selected-model" is name of the selected AI model, and "temp-setting" is the numeric temperature
            setting defined by the user.

            The content section of a message should be replaced with the user's message to the AI.

            With streaming enabled, responses arrive as server-sent events. Each event begins with "data: " followed by
            event type and a JSON object. The sequence will be:

            1. message_start with initial usage statistics
            2. content_block_start to mark beginning of response
            3. content_block_delta events with actual content
            4. content_block_stop for end of current block
            5. message_delta with final usage statistics
            6. message_stop to end the sequence

            Example response chunk formats:

            ```json
            {
                "type": "content_block_delta",
                "index": 0,
                "delta": {
                    "type": "text_delta",
                    "text": "chunk of text"
                }
            }
            ```

            Usage stats are provided in message_start and message_delta events:

            ```json
            {
                "usage": {
                    "input_tokens": 12,
                    "output_tokens": 6
                }
            }
            ```

            Context: Provider-specific errors
                Context: Anthropic errors
                    Anthropic errors include:

                    - Invalid API key
                    - Overloaded error (too many requests)
                    - Model overloaded

    Context: Logging
        Where appropriate the application will log key operations, errors, and exceptions to a log file.  A new log file
        will be created at the start of each application run.

        The log file will be named "yyyy-mm-dd-hh-mm-ss-ttt.log", substituting the application start date and time (in UTC)
        for yyyy, mm, dd, hh, mm, ss, ttt (year, numeric month, day-of-month, hours, minutes, seconds, and thousandths of
        seconds respectively).

        The log file will be written to a "logs" directory of the ".humbug" directory in the user's home directory.  If this
        does not exist then the application should create it.

        The application will maintain the last 50 log files but delete any older ones.

        If any log file exceeds 1 MByte in size then it will be rotated.

    Context: GUI
        The GUI must be built using the latest version of PySide6, and with qasync to support the integration of this
        and async IO operations.

        Note that PySide6 correctly translates the keyboard shortcuts described into appropriate platform-specific
        shortcuts and no extra logic is required for these.

        Context: Platform support
            The GUI must work on MacOS X (any version since 2020), Linux (any version since 2020), and Microsoft
            Windows 10 or 11.

        Context: Asynchronous design
            The UI must be asynchronous to ensure the application can remain reactive.

        Context: Performance guidelines
            Context: Scrolling performance
                - 60 FPS target for scroll operations

    Context: Python implementation and dependencies
        As an engineer working with the application, I want the application to be easy to use and understand,
        so I can maintain and enhance it over time.

        Context: Implement in Python 3
            The application will be written in the latest version of Python 3.

        Context: Indentation of code
            Code must be indented by 4 spaces.

        Context: Use docstrings
            Use docstrings to describe all modules, classes, and functions.  This should follow PEP 257 guidelines.

        Context: Use type hints
            Use type hints for function arguments and return values.

        Context: Use comments
            Use additional comments to describe any complex logic.

        Context: PEP 8 imports
            The import list in any module should follow PEP 8 guidelines, including the ordering of imports.

        Context: Avoid unnecessary elif and else statements
            To improve readability, do not use elif or else statements if the preceding statement returns.

            For example, do this:

            ```python
            if condition:
                return

            next_statement()
            ```

            instead of this:

            ```python
            if condition:
                return
            else:
                next_statement()
            ```

        Context: Dependencies
            Leverage standard library tools before custom solutions, unless specifically instructed.

            Context: HTTP interations
                Use the aiohttp library for HTTP interactions (e.g. the REST API).

            Context: Metaphor compilation
                Use the m6rclib library for Metaphor file compilation.

        Context: Exception handling philosophy
            Context: Exception documentation
                Document what exceptions each function may raise in its docstring.

            Context: Handling exceptions
                We should attempt to handle and mitigate exceptions at the level closest to which they are first
                detected.  If we cannot handle or mitigate the exception then it should be wrapped in a domain-appropriate
                exception class and this re-raised to the next level up the call stack.

                Include contextual information when wrapping exceptions.

                Preserve the exception chain using "raise ... from e" syntax.

            Context: Avoid bare "except:" or "except Exception:" clauses.
                We should avoid the use of bare "except:" or "except Exception:" clauses unless a function we are calling
                can only have exceptions handled this way.

                We should always catch specific exception types that could occur from an operation.

            Context: Exception logging
                All exceptions should be logged when they occur, before re-raising or wrapping them.

                Use appropriate log levels:
                - ERROR for exceptions that indicate failure.
                - WARNING for exceptions that can be handled/recovered from.
                - DEBUG for detailed exception information during development.

                We must include sufficient context in log messages to aid debugging.

            Context: Exception wrapping example
                Do this:

                ```python
                try:
                    await self.api_client.fetch_data()
                except ConnectionError as e:
                    logger.error("Failed to retrieve data from API endpoint", exc_info=True)
                    raise DataFetchError(f"Failed to retrieve data from API endpoint: {e}") from e
                except TimeoutError as e:
                    logger.warning("API request timed out, will retry", exc_info=True)
                    # Handle retry logic
                ```

                Not this:

                ```python
                try:
                    await self.api_client.fetch_data()
                except Exception as e:
                    logger.error(f"Error: {e}")  # Insufficient context, no stack trace
                    raise  # No wrapping or additional context
                ```
