Role:
    You are a world-class python programmer with a flair for building brilliant software

Context: humbug application
    humbug is a GUI-based application that allows a user to interact with one or more AI backends.

    Navigation is done using a keyboard and mouse.

    Context: Version
        This is version 0.1 of the application.

    Context: Screen display
        The primary screen display is a tabbed interface, where the tabs correspond to different conversations being
        had with one or more AI backends.

        Selecting a tab brings that conversation to the front of the tab group.

        Context: Conversation tabs
            Each tab will be named with a conversation name.  This will default to "Conv <num>" where <num> is a
            simple incrementing number starting from 1 for each conversation created during the current run of the
            application (e.g., "Conv 1", "Conv 2", "Conv 3", etc.).  These numbers do not need to persist between
            different runs of the application.

            Each tab will have a close button (typically rendered as an "x" icon) that appears after the conversation
            name.  Clicking this close button will close the tab in the same way as clicking the "Close Conversation"
            menu item.

            The close button will be:

            - Always visible but not highlighted on the currently selected tab.
            - Hidden by default on non-selected tabs, appearing only when the tab is hovered over.
            - Highlighted in red when the mouse hovers over the close button itself.

        Context: History display
            Each message in the history appears as a distinct "card" that spans the full width of the history view.

            Each card has:

            - A header section with a dark bluish-grey (#2a3544) background containing the sender name
                ("You", "Assistant", or "System Message")
            - A content section with the message text that uses appropriate background colors:
                - User messages: White text on #3c3c3c background
                - AI responses: White text on #282828 background
                - System messages: White text on #1a3a1a background (dark green)
                - Error messages: White text on #3a1a1a background (dark red)
            - A border in the same color as the header (#2a3544)
            - 8px spacing between cards
            - 8px padding within the content area

            The cards dynamically resize with the window:

            - The width always matches the window width (minus margins and scroll bar)
            - The height adjusts automatically based on the content
            - Text wraps within the card boundaries
            - Code blocks maintain their formatting while wrapping where possible

            Context: Example layout
                Here's an ASCII representation of the layout:

                ```plaintext
                +------------------------------------------+
                |  You                                     |
                |------------------------------------------|
                |  This is a user message that wraps       |
                |  across multiple lines as needed.        |
                +------------------------------------------+
                                ↕ 10px
                +------------------------------------------+
                |  Assistant                               |
                |------------------------------------------|
                |  This is an AI response that also wraps  |
                |  across multiple lines as needed.        |
                |                                          |
                |  ```python                              |
                |  def example():                         |
                |      print("Code blocks maintain their   |
                |            formatting")                  |
                |  ```                                    |
                +------------------------------------------+
                                ↕ 10px
                +------------------------------------------+
                |  System Message                          |
                |------------------------------------------|
                |  This is a system message that wraps     |
                |  across multiple lines as needed.        |
                +------------------------------------------+
                ```

    Context: Menus
        The application will have a series of top-level Menus.

        Where any action cannot be performed at the current time it will be greyed out.

        Context: Humbug
            The "Humbug" menu is the main application menu.  When clicked it contains the following:

            Context: About Humbug
                The "About Humbug" menu item will deliver a modal pop-up dialog box that gives details of the
                application and version information.

            Context: Separator
                There will be a horizontal Separator

            Context: Quit Humbug
                The "Quit Humbug" menu item will allow the user to exit the application.

        Context: File
            The "File" menu handles new files and tabs.  When clicked it contains the following:

            Context: New Conversation
                The "New Conversation" menu item will create a new conversation tab.

            Context: Close Conversation
                The "Close Conversation" menu item with close the current conversation, if a conversation is active.

        Context: Edit
            The "Edit" menu handles edit capabilities.  When clicked it contains the following:

            Context: Submit
                The "Submit" menu item will submit the current user input message to the AI, where there is
                an input message to be submitted.

                This will be matched with a keyboard shortcut, "Ctrl+J".

            Context: Separator
                There will be a horizontal Separator

            Context: Undo
                The "Undo" menu item will all the last user input action that has not yet been committed (e.g.
                sent to the AI) to be undone.

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+Z" for Windows).

            Context: Redo
                The "Redo" menu item will restore previos undo operations, where the last user input action has
                not yet been committed (e.g. sent to the AI) to be undone.

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+Shift+Z" for Windows).

            Context: Separator
                There will be a horizontal Separator

            Context: Cut
                The "Cut" menu item will support the cutting of input text from a highlighted area where such
                text can be removed.

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+X" for Windows).

            Context: Copy
                The "Copy" menu item will support the copying of text from a highlighted area.

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+C" for Windows).

            Context: Paste
                The "Paste" menu item will support the pasting of text at the current input cursor, where
                new text can be added.

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+V" for Windows).

            Context: Paste behavior
                When text is pasted into the input area via any method (menu, keyboard shortcuts, or right-click),
                only the plain text content is transferred.  All formatting from the source text (including fonts,
                colors, and text styling) is removed, ensuring the pasted content matches the default input area
                styling.

            Context: Separator
                There will be a horizontal Separator

            Context: Conversation Settings
                The "Conversation Settings" menu item will support setting the conversation settings of
                the currently focused conversation tab (if there is one).

                This will be matched with a standard keyboard shortcut (e.g. "Ctrl+," for Windows).

        Context: View
            The "View" menu handles view settings.  When clicked it contains the following:

            Context: Zoom In
                The "Zoom In" menu item will support zooming in and making the contents of the application
                larger.  All visual elements must increase in size by a factor of 1.189027, up to a maximum
                of 2.0

                This will be matched with a keyboard shortcut (e.g. "Ctrl+=" for Windows).

            Context: Zoom Out
                The "Zoom Out" menu item will support zooming out and making the contents of the application
                smaller.  All visual elements must decrease in size by a factor of 1.189027, down to a minimum
                of 0.5

                This will be matched with a keyboard shortcut (e.g. "Ctrl+-" for Windows).

            Context: Reset Zoom
                The "Reset Zoom" menu item will restore the zoom level of the application to its default 1.0
                of 0.5

                This will be matched with a keyboard shortcut (e.g. "Ctrl+0" for Windows).

    Context: User/AI interactions
        Each conversation comprises a chat between a user and one or more AI backends.

        If the user starts a message with "/" this indicates a command to the application rather than a message
        for the AI.  The application must process commands as they are presented, stripping the "/" before
        processing..

        Interactions occur in a turn-by-turn fashion, where the user edits a message, until they hit "Ctrl+J".  On
        hitting "Ctrl+J" the application will add the user's message to the conversation transcript and process the
        message.  If the message is for the AI then the application will attempt to send the message to the AI
        and await a response.  If there is an error or exception then the application will report the details to
        the user.

    Context: Conversations
        The bottom line of each conversation window has a status bar.  It will show the current numbers of input and
        output tokens that have been used since the start of the application.

        The top part of the conversation window will show a history of everything that has happened, including
        messages and commands from the user, responses from the application, and responses from the AI.  Historical
        messages cannot be edited

        Below the conversation history and above the status bar is a user input editor.  This input area will increase
        in size upwards as the user adds more content.  This editable region can be arbitrarily large.

        To the user, the historical details and the editable region must feel like they're one window.  Where they
        are required, there is only one vertical scroll bar to cover both the history and editable area.

        Context: Highlighting operations
            While the editable and non-editable regions must feel like they're one thing, if the user wants to
            interact with them they will behave differently.  For example, highlighting of text cannot span
            between the editable and non-editable regions.

        Context: Screen colours
            The status line background is light grey with black text.

            Message backgrounds and text colors are:
                - User messages: White text on #3c3c3c background
                - AI responses: White text on #282828 background
                - System messages: White text on #1a3a1a background (dark green)
                - Error messages: White text on #3a1a1a background (dark red)

        Context: Conversation Settings
            Each conversation can have its own AI model and temperature settings that affect how the AI responds.

            Context: Settings Dialog
                A "Settings" dialog can be accessed for each conversation via a menu or keyboard shortcut (see
                "Conversation Settings" menu item)

                The dialog shows the current settings for the active conversation tab.

                Context: Model Selection
                    The AI model is selected via a dropdown menu.
                    Available models should be configurable at application startup.
                    The default model is "gpt-4o-mini".
                    Any change to the model selection takes effect on the next message sent to the AI.

                Context: Temperature Setting
                    The temperature is set via a numeric input field that:

                    - Accepts values between 0.0 and 1.0
                    - Shows one decimal place
                    - Has spinner arrows for adjustment
                    - Default value is 0.7
                    - Changes take effect on the next message sent to the AI

                Context: Dialog Controls
                    The dialog has:

                    - "OK" button to save changes and close
                    - "Cancel" button to discard changes and close
                    - "Apply" button to save changes without closing

                    The dialog is modal, preventing interaction with the main window while open.

        Context: UI navigation
            Context: Input box navigation
                If the input area has not yet been edited and the cursor is at the top left of the input area, then
                the user can pull up previous historical messages to the AI by using the up and down keys.

        Context: Input formatting
            The input box supports Markdown-style code formatting:

            Context: Code blocks
                Text between matching triple backticks (```) is displayed as a code block.
                Code blocks use a monospace font (set as a family fallback: Menlo, Monaco, Courier New, monospace).
                The background color for code blocks is dark grey (#2d2d2d) and extends across the full width of the input box.
                The code block formatting applies to both the triple backtick lines and all content between them.

            Context: Inline code
                Text between single backticks (`) is displayed as inline code.
                Inline code uses the same monospace font as code blocks.
                Inline code has no background color.

            Context: Formatting updates
                The formatting is updated in real-time as the user types.
                When inside a code block, each new line is automatically formatted with the monospace font and background.
                The full width background for code blocks is applied immediately, and not just when a newline is entered.

        Context: History display formatting
            The history display supports the same Markdown-style code formatting as the input box:

            Context: Code blocks
                Text between matching triple backticks (```) is displayed as a code block.
                Code blocks use the same monospace font as the input area (Menlo, Monaco, Courier New, monospace).
                The background color for code blocks is dark grey (#2d2d2d) and extends across the full width.
                The code block formatting applies to both the triple backtick lines and all content between them.

            Context: Inline code
                Text between single backticks (`) is displayed as inline code.
                Inline code uses the same monospace font as code blocks.
                Inline code has no background color.

            Context: Formatting interaction
                The Markdown formatting is applied in real-time as messages are added or updated.

    Context: Conversation transcript file
        The application will record a transcript of all conversations between the user and the AI, storing each turn
        in a JSON data structure.  The JSON data structure will be written to a transcript file called
        "transcript-yyyy-mm-dd-hh-mm-ss.json", substituting the application start date and time (in GMT) for yyyy,
        mm, dd, hh, mm, and ss (year, numeric month, day-of-month, hours, minutes, and seconds respectively).

        On exiting the application this file must be fully closed.

        Context: Transcript JSON schema
            Use the following JSON schema:

            ```json
            {
                "$schema": "http://json-schema.org/draft-07/schema#",
                "type": "object",
                "properties": {
                    "metadata": {
                        "type": "object",
                        "properties": {
                            "timestamp": { "type": "string", "format": "date-time" },
                            "version": { "type": "string" },
                            "conversation_number": { "type": "integer" }
                        },
                        "required": ["timestamp", "version", "conversation_number"]
                    },
                    "conversation": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "id": { "type": "string", "pattern": "^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$" },
                                "timestamp": { "type": "string", "format": "date-time" },
                                "type": { "enum": ["user_message", "ai_response", "command", "system_message"] },
                                "content": { "type": "string" },
                                "model": {
                                    "type": "string",
                                    "description": "AI model used (only present for ai_response type)"
                                },
                                "temperature": {
                                    "type": "number",
                                    "minimum": 0,
                                    "maximum": 1,
                                    "description": "Temperature setting used (only present for ai_response type)"
                                },
                                "usage": {
                                    "type": "object",
                                    "properties": {
                                        "prompt_tokens": { "type": "integer" },
                                        "completion_tokens": { "type": "integer" },
                                        "total_tokens": { "type": "integer" }
                                    }
                                },
                                "error": {
                                    "type": "object",
                                    "properties": {
                                        "code": { "type": "string" },
                                        "message": { "type": "string" },
                                        "details": { "type": "object" }
                                    }
                                }
                            },
                            "required": ["id", "timestamp", "type", "content"]
                        }
                    }
                },
                "required": ["metadata", "conversation"]
            }
            ```

    Context: Transcript file management
        Context: Transcript Rotation
            - Max file size: 50MB.
            - On size limit: Create new file with incremented suffix.
            - Keep last 5 transcript files.
            - Delete oldest when limit reached.

        Context: Error recovery
            - Write to temp file first.
            - Atomic rename on successful write.
            - On write failure:
                1. Log error to stderr.
                2. Create backup file.
                3. Continue with new file.

    Context: Commands
        The following are commands the application must support (commands are case sensitive):

        Context: "exit":
            On receiving the "exit" command the application will exit.

        Context: Unknown commands
            If the user types and unknown command then the application will respond that it does not know the
            command.

    Context: State handling
        No state is preserved between different invocations of the application.

    Context: AI interactions
        The AI backend supports streaming responses that arrive incrementally.  These should be displayed
        as they arrive with content continuing to be added as more arrives.

        Context: Streaming responses
            The backend provides chunks containing:

            - Content so far
            - Usage statistics (in final chunk only)

            Streaming requires that the display:

            - Shows content immediately as it arrives
            - Updates transcript periodically during streaming
            - Writes final response when stream completes
            - Updates token count after stream completes

            During streaming, if the user hits "Esc" then this will terminate the current streaming response and cancel
            any ongoing request with the AI.  A message "Request cancelled by user" will be added to the history.

        Context: UI handling of streaming
            The display must handle streaming responses by:

            - Tracking where the streamed response starts in history
            - Updating display incrementally as content arrives
            - Removing previous content before adding updates
            - Maintaining newline format and text wrapping

    Context: Communication with the AI
        Communication with the AI is via a REST API.

        The initial implementation solely supports OpenAI.

        Context: OpenAI REST endpoint
            Messages are sent via a POST message.
            The API to use is the "continuations" API.  The endpoint is "https://api.openai.com/v1/chat/completions".
            This needs two headers to be provided:
            "Content-Type": "application/json"
            "Authorization": {key}"
            {key} must be replaced with the API key found in the environment variable `OPENAI_API_KEY`.

            The data for the POST is of this form:

            ```json
            {
                "model": "selected-model",
                "messages": [{"role": "user", "content": "Say this is a test!"}],
                "temperature": temp-setting,
                "stream": true,
                "stream_options": {"include_usage": true}
            }
            ```

            In this, "selected-model" is name of the selected AI model, and "temp-setting" is the numeric temperature
            setting defined by the user.

            The "content" section of a message should be replaced with the user's message to the AI.
            With streaming enabled, responses will arrive as chunks. Each chunk begins with "data: " followed
            by a JSON object. This repeats until a chunk containing "data: [DONE]" arrives.  Each JSON chunk
            takes this form:

            ```json
            {
                "id": "chatcmpl-abc123",
                "object": "chat.completion.chunk",
                "created": 1677858242,
                "model": "gpt-4o-mini",
                "choices": [
                    {
                        "delta": {
                            "content": "chunk of text"
                        },
                        "finish_reason": null,
                        "index": 0
                    }
                ]
            }
            ```

            The final chunk includes usage information:

            ```json
            {
                "id": "chatcmpl-abc123",
                "object": "chat.completion.chunk",
                "created": 1677858242,
                "model": "gpt-4o-mini",
                "usage": {
                    "prompt_tokens": 13,
                    "completion_tokens": 7,
                    "total_tokens": 20
                },
                "choices": [
                    {
                        "delta": {},
                        "finish_reason": "stop",
                        "index": 0
                    }
                ]
            }
            ```

            In this message, the content is what should be captured as the AI response, but capture the usage information
            in the JSON transcript.

    Context: Message history
        Maintain conversation context by including all previous messages in the session.

    Context: AI interactions
        The AI model is configurable via the settings dialog.  It it set to "gpt-4o-mini" by default.

        Where adjustable, the AI temperature for an AI model will be configurable via the settings dialog.

        Context: Available models
            The following AI models are available:

            - "gpt-4o-mini"
            - "gpt-4o"
            - "o1-mini"
            - "o1-preview"

    Context: Temperature settings
        Temperature controls how deterministic or creative the AI's responses are.

        GPT-4o models support temperature adjustment:

        - gpt-4o-mini: Supports temperature 0.0-1.0.
        - gpt-4o: Supports temperature 0.0-1.0.
        - o1-mini: No temperature setting.
        - o1-preview: No temperature setting.

        The temperature control in the Settings dialog is automatically:

        - Enabled and set to 0.7 by default for GPT-4o models.
        - Disabled for models that do not support a temperature setting.
        - Preserves previous temperature when switching between models that support temperatures.
        - Reverts to default when switching from models that do not support temperatures to a model that does.

    Context: Error handling and recovery
        Context: Handling other network errors
            If there are any API errors, such as invalid API keys then the application should report these to the user
            so they are aware these need to be corrected.

    Context: Error handling and recovery
        Context: Error Categories
            The application handles these error types:

            - Network: Connection, timeout, DNS failures
            - API: Authentication, rate limits, invalid requests
            - Application: Input validation, state corruption
            - System: File IO, memory, permissions

        Context: Network timeouts
            If the AI does not respond within 20 seconds of being sent a request, then the application should treat this as
            a timeout and retry up to 3 times.  Each time it retries, it should report that it is retrying.

        Context: Error response strategy
            For each error:

            - Log full details to transcript
            - Display user-friendly message as an application response.  For example:

                429: "Rate limit exceeded.  Retrying in [X] seconds..."
                500: "OpenAI service error.  Retrying in [X] seconds..."
                timeout: "Request timed out.  Retrying in [X] seconds..."

            - Implement appropriate recovery

            Recovery approach:

            - Network errors: Retry with exponential backoff.  Initial delay 2 seconds, doubling each retry.  Max 3 retries.
            - API errors: Handle rate limits, refresh credentials
            - Application errors: Reset to known good state
            - System errors: Graceful degradation

            During retries, preserve the message in the input box.

        Context: Error display format
            Errors appear in message history with:

            - Severity indicator: [ERROR], [WARNING], [INFO]
            - Clear action message if user intervention needed
            - Technical details in expandable section

        Context: Recovery automation
            Context: Rate Limiting
                Track API calls with sliding 60-second window.

                Pause requests when approaching limits.

            Context: Retry strategy
                Maximum 3 retries for recoverable errors.

                Exponential backoff: 2, 4, 8 seconds.

                Preserve conversation context between retries.

    Context: GUI
        The GUI must be built using the latest version of PySide6, and with qasync to support the integration of this
        and async IO operations.

        Note that PySide6 correctly translates the keyboard shortcuts described into appropriate platform-specific
        shortcuts and no extra logic is required for these.

        Context: Platform support
            The GUI must work on MacOS X (any version since 2020), Linux (any version since 2020), and Microsoft
            Windows 10 or 11.

        Context: Asynchronous design
            The UI must be asynchronous to ensure the application can remain reactive.

        Context: Performance guidelines
            Context: Scrolling performance
                - 60 FPS target for scroll operations

    Context: Security
        Context: Outputs
            Ensure that no sensitive data is either logged or displayed on the screen.

            For example, API keys must never be revealled this way - replace with "[redacted API key]".

        Context: Inputs
            Ensure that all input control characters except for newlines are stripped from any input prior to processing.

    Context: Python implementation and dependencies
        As an engineer working with the application, I want the application to be easy to use and understand,
        so I can maintain and enhance it over time.

        Context: Implement in Python 3
            The application will be written in the latest version of Python 3.

        Context: Indentation of code
            Code must be indented by 4 spaces.

        Context: Use docstrings
            Use docstrings to describe all modules, classes, and functions.  This should follow PEP 257 guidelines.

        Context: Use type hints
            Use type hints for function arguments and return values.

        Context: Use comments
            Use additional comments to describe any complex logic.

        Context: PEP 8 imports
            The import list in any module should follow PEP 8 guidelines, including the ordering of imports.

        Context: Avoid unnecessary elif and else statements
            To improve readability, do not use elif or else statements if the preceding statement returns.

            For example, do this:

            ```
            if condition:
                return

            next_statement()
            ```
            instead of this:
            ```
            if condition:
                return;
            else:
                next_statement()
            ```

        Context: Dependencies
            Leverage standard library tools before custom solutions, unless specifically instructed.

            Context: HTTP interations
                Use the aiohttp library for HTTP interactions (e.g. the REST API).

        Context: Exception handling philosophy
            Context: Exception documentation
                Document what exceptions each function may raise in its docstring.

            Context: Handling exceptions
                We should attempt to handle and mitigate exceptions at the level closest to which they are first
                detected.  If we cannot handle or mitigate the exception then it should be wrapped in a domain-appropriate
                exception class and this re-raised to the next level up the call stack.

                Include contextual information when wrapping exceptions.

                Preserve the exception chain using "raise ... from e" syntax.

            Context: Avoid bare "except:" or "except Exception:" clauses.
                We should avoid the use of bare "except:" or "except Exception:" clauses unless a function we are calling
                can only have exceptions handled this way.

                We should always catch specific exception types that could occur from an operation.

            Context: Exception logging
                All exceptions should be logged when they occur, before re-raising or wrapping them.

                Use appropriate log levels:
                - ERROR for exceptions that indicate failure.
                - WARNING for exceptions that can be handled/recovered from.
                - DEBUG for detailed exception information during development.

                We must include sufficient context in log messages to aid debugging.

            Context: Exception wrapping example
                Do this:

                ```python
                try:
                    await self.api_client.fetch_data()
                except ConnectionError as e:
                    logger.error("Failed to retrieve data from API endpoint", exc_info=True)
                    raise DataFetchError(f"Failed to retrieve data from API endpoint: {e}") from e
                except TimeoutError as e:
                    logger.warning("API request timed out, will retry", exc_info=True)
                    # Handle retry logic
                ```

                Not this:

                ```python
                try:
                    await self.api_client.fetch_data()
                except Exception as e:
                    logger.error(f"Error: {e}")  # Insufficient context, no stack trace
                    raise  # No wrapping or additional context
                ```

Action: Build the software
    Please review the requirements provided in the Context section and check if they are met by the software.  Do not
    make modifications to the software at this point, unless asked to do so.
#    Please review the requirements provided in the Context section and build the software described.  Take care to
#    address all the behaviours asked for and do not omit anything.
#
#    Do not produce any other commentary other than the code.
#
#    If the software should be structured into multiple files then please provide each file separately and identify the
#    name of each one as you produce it.

    An earlier version of the application is provided here.  Please use this as a template.  This version may not meet
    all the requirements provided in the Context section, so you may need to add or remove code to meet the full set of
    requirements specified.

    Embed: src/humbug/**/*.py
